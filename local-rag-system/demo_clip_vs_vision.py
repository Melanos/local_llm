#!/usr/bin/env python3
"""
Demonstrate CLIP's multimodal capabilities vs image-to-text generation
"""

import sys
from pathlib import Path
import os

# Add project root to path
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

try:
    from src.core.enhanced_rag_engine import EnhancedRAGEngine
    from config import DEFAULT_CONFIG
except ImportError as e:
    print(f"âŒ Import error: {e}")
    sys.exit(1)

def demonstrate_clip_capabilities():
    """Show what CLIP can and cannot do"""
    print("ğŸ” CLIP Multimodal Capabilities Demo")
    print("=" * 50)
    
    # Initialize CLIP engine
    print("ğŸš€ Initializing CLIP engine...")
    engine = EnhancedRAGEngine(embedding_model_key="clip")
    
    if not engine.supports_images:
        print("âŒ CLIP not available")
        return
    
    print("âœ… CLIP engine initialized")
    print(f"ğŸ–¼ï¸  Multimodal support: {engine.supports_images}")
    
    # Look for test images
    image_dir = project_root / "data" / "images"
    if not image_dir.exists():
        print("âŒ No image directory found")
        return
    
    image_files = list(image_dir.glob("*.png")) + list(image_dir.glob("*.jpg")) + list(image_dir.glob("*.jpeg"))
    
    if not image_files:
        print("âŒ No images found")
        return
    
    test_image = str(image_files[0])
    print(f"ğŸ“· Testing with: {os.path.basename(test_image)}")
    
    print("\nğŸ” What CLIP CAN do:")
    print("-" * 30)
    
    # 1. Generate image embedding
    print("1ï¸âƒ£ Generate semantic embedding from image...")
    image_embedding = engine.embedding_engine.embed_image(test_image)
    if image_embedding:
        print(f"âœ… Created 512-dimensional embedding")
        print(f"ğŸ“Š First 5 values: {image_embedding[:5]}")
    
    # 2. Add image to database
    print("\n2ï¸âƒ£ Add image to searchable database...")
    success = engine.add_image(test_image, {"description": "Network diagram"})
    if success:
        print("âœ… Image added to vector database")
    
    # 3. Search with text queries
    print("\n3ï¸âƒ£ Search images using text queries...")
    text_queries = [
        "network diagram",
        "computer infrastructure", 
        "technical drawing",
        "servers and routers"
    ]
    
    for query in text_queries:
        results = engine.search_multimodal(query, n_results=3)
        if results:
            similarity = results[0].get('similarity', 0)
            result_type = results[0].get('type', 'unknown')
            print(f"   ğŸ” '{query}' â†’ {result_type} (similarity: {similarity:.4f})")
        else:
            print(f"   âŒ '{query}' â†’ No results")
    
    # 4. Cross-modal similarity
    print("\n4ï¸âƒ£ Compare text and image embeddings...")
    text_embedding = engine.embedding_engine.embed_query("network topology diagram")
    if text_embedding and image_embedding:
        # Calculate cosine similarity
        import numpy as np
        text_vec = np.array(text_embedding)
        image_vec = np.array(image_embedding)
        similarity = np.dot(text_vec, image_vec) / (np.linalg.norm(text_vec) * np.linalg.norm(image_vec))
        print(f"âœ… Text-image similarity: {similarity:.4f}")
    
    print("\nâŒ What CLIP CANNOT do:")
    print("-" * 30)
    print("âŒ Generate text descriptions from images")
    print("âŒ Answer questions about image content") 
    print("âŒ Create captions or detailed descriptions")
    print("âŒ Identify specific objects or text in images")
    
    print("\nğŸ’¡ For Image-to-Text Generation, you'd need:")
    print("-" * 45)
    print("ğŸ”§ LLaVA (recommended for local use)")
    print("ğŸ”§ InstructBLIP")
    print("ğŸ”§ BLIP-2") 
    print("ğŸ”§ GPT-4 Vision API")
    print("ğŸ”§ Vicuna with vision capabilities")

if __name__ == "__main__":
    demonstrate_clip_capabilities()
